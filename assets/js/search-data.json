{
  
    
        "post0": {
            "title": "Hands-on AI for Non-Coders: Image Classification Basics for Beginners",
            "content": ". . Introduction . In this demonstration, we will utilize techniques of computer vision, including deep convolutional neural networks (CNNs), to train an image classifier model capable of classifying radiographs as either chest or abdominal. . Code . We will utilize the fast.ai v2 library, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the Python programming language and built on top of the PyTorch deep learning library. . The demonstration in this notebook relies heavily on examples from the fast.ai book, Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are freely available for download on GitHub. A print copy of the book can be purchased from Amazon. . Data . This work is adapted from &quot;Hello World Deep Learning in Medical Imaging 1&quot;. The chest and abdominal radiographs were obtained from Paras Lakhani&#39;s GitHub repository. . 1. Reference: Lakhani P, Gray DL, Pett CR, Nagy P, Shih G. Hello World Deep Learning in Medical Imaging. J Digit Imaging. 2018 Jun; 31(3):283-289. Published online 2018 May 3. doi: 10.1007/s10278-018-0779-6↩ . Developers . Walter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA | Kirti Magudia, MD, PhD, - University of California, San Francisco, CA, USA | M. Travis Caton, MD, PhD - University of California, San Francisco, CA, USA | . Acknowledgements . Other versions of this notebook implemented on the Kaggle Notebooks platform were presented at the 2019 Society for Imaging Informatics in Medicine (SIIM) Annual Meeting and for the American College of Radiology (ACR) Residents &amp; Fellows Section (RFS) AI Journal Club. . We would also like to acknowledge the following individuals for inspiring our transition to the Google Colab platform with their excellent notebook from the 2019 RSNA AI Refresher Course: . Luciano M. Prevedello, MD, PhD | Felipe C. Kitamura, MD, MSc | Igor Santos, MD | Ian Pan, MD | . System Setup &amp; Downloading the Data . . Important: Save a copy of this notebook in your Google Drive folder by selecting Save a Copy in Drive from the File menu in the top left corner of this page. This will allow you to modify the cells and save your results. . Warning: Make sure you have the runtime type set to &quot;GPU&quot;. See GIF below. . . Setting up the runtime environment... . Running the following cell in Colab will install the necessary libraries, download the data and restart the session. . Warning: This will generate an error message, which we can safely ignore 😉. . import os !pip install fastai==2.1.4 &gt;/dev/null !pip install fastcore==1.3.1 &gt;/dev/null # **Downloading the data...** !wget -q https://github.com/wfwiggins/RSNA-Image-AI-2020/blob/master/data.zip?raw=true !mkdir -p data !unzip -o data.zip?raw=true -d data &gt;/dev/null !rm data.zip?raw=true os.kill(os.getpid(), 9) . Exploring the Data . Let&#39;s take a look at the directory structure and contents, then create some variables to help us as we proceed. . import warnings warnings.simplefilter(&#39;ignore&#39;) import matplotlib.pyplot as plt plt.rcParams[&quot;figure.figsize&quot;] = (12, 12) . . from fastai.basics import * from fastai.vision.all import * # Set path variable to the directory where the data is located path = Path(&#39;/content/data&#39;) # Command line &quot;magic&quot; command to show directory contents !ls {path}/**/* . /content/data/test/abd: abd_test.png /content/data/test/chest: chest_test.png /content/data/train/abd: abd0.png abd14.png abd19.png abd23.png abd28.png abd3.png abd8.png abd10.png abd15.png abd1.png abd24.png abd29.png abd4.png abd9.png abd11.png abd16.png abd20.png abd25.png abd2.png abd5.png abd12.png abd17.png abd21.png abd26.png abd30.png abd6.png abd13.png abd18.png abd22.png abd27.png abd31.png abd7.png /content/data/train/chest: chst33.png chst39.png chst45.png chst51.png chst57.png chst63.png chst34.png chst40.png chst46.png chst52.png chst58.png chst64.png chst35.png chst41.png chst47.png chst53.png chst59.png chst65.png chst36.png chst42.png chst48.png chst54.png chst60.png chst37.png chst43.png chst49.png chst55.png chst61.png chst38.png chst44.png chst50.png chst56.png chst62.png /content/data/val/abd: abd0.png abd1.png abd2.png abd3.png abd4.png /content/data/val/chest: chst0.png chst1.png chst2.png chst3.png chst4.png . As you can see, the data directory contains subdirectories train, val and test, which contain the training, validation and test data for our experiment. train and val contain subdirectories abd and chest containing abdominal and chest radiographs for each data set. There are 65 training images and 10 validation images with balanced distributions over our target classes (i.e. approximately equal numbers of abdominal and chest radiographs in each data set and optimized for a classification problem). . Model Training Setup . Before we train the model, we have to get the data in a format such that it can be presented to the model for training. . Data Loaders . The first step is to load the data for the training and validation datasets into a ImageDataLoaders object from the fastai library. When training a model, the ImageDataLoaders will present training - and subsequently, validation - data to the model in batches. . Data Augmentation . In order to be sure that the model isn&#39;t simply &quot;memorizing&quot; the training data, we will augment the data by randomly applying different transformations to each image before it is sent to the model. . Transformations can include rotation, translation, flipping, rescaling, etc. . Load the data into ImageDataLoaders with data augmentation . . Note: When you run this next cell in Colab, a batch of data will be shown with or without augmentation transforms applied. (1) Run this cell once with the box next to apply_transforms unchecked to see a sample of the original images. (2) Next, run the cell a few more times after checking the box next to apply_transforms to see what happens to the images when the transforms are applied. . apply_transforms = True #@param {type: &#39;boolean&#39;} if apply_transforms: flip = True max_rotate = 10.0 max_warp = 0.2 p_affine = 0.75 else: flip = False max_rotate, max_warp, p_affine = 0, 0, 0 tfms = aug_transforms( do_flip=flip, max_rotate=max_rotate, max_warp=max_warp, p_affine=p_affine, size=224, min_scale=0.75 ) dls = ImageDataLoaders.from_folder(path, valid=&#39;val&#39;, seed=42, item_tfms=Resize(460), batch_tfms=tfms, bs=16) dls.show_batch(max_n=6) . Find the optimal learning rate . The learning rate is a hyperparameter that controls how much your model adjusts in response to percieved error after each training epoch. Choosing an optimal learning rate is an optimal step in model training. . From the fastai docs: . First introduced by Leslie N. Smith in Cyclical Learning Rates for Training Neural Networks, the LRFinder trains the model with exponentially growing learning rates and stops in case of divergence.&gt; The losses are then plotted against the learning rates with a log scale. A good value for the learning rates is then either:&gt; - 1/10th of the minimum before the divergence&gt; - where the slope is the steepest . . Note:When you run this cell for the first time in a Colab session, it will download a pretrained version of the model to your workspace before running the LRFinder. . dls = ImageDataLoaders.from_folder(path, valid=&#39;val&#39;, seed=42, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75), bs=16) learn = cnn_learner(dls, resnet18, metrics=accuracy) learn.lr_find(); . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . Transfer Learning . Deep learning requires large amounts of training data to successfully train a model. . When we don&#39;t have enough data to work with for the planned task, starting with a pre-trained network that has been optimally trained on another task can be helpful. The concept of re-training a pre-trained network for a different task is called transfer learning. . Fine-tuning . In the process of re-training the model, we start by changing the final layers of the network to define the output or predictions our model will make. In order to avoid propagating too much error through the rest of the network during the initial training, we freeze the other layers of the network for the first cycle or epoch of training. Next, we open up the rest of the network for training and train for a few more epochs. This process is called fine-tuning. . Epochs and data augmentation . During each epoch, the model will be exposed to the entire dataset. Each batch of data will have our data transformations randomly applied in order to provide data augmentation. This helps to ensure that our model never sees the exact same image twice. This is important because we wouldn&#39;t want our model to simply memorize the training dataset and not converge on a generalized solution, resulting in poor performance on the validation dataset. . The loss function . In a classification task, you&#39;re either right or wrong. This binary information doesn&#39;t give us much nuance to work with when training a model. A loss function give us a numeric estimation of &quot;how wrong&quot; our model is. This gives us a target to optimize during the training process. . When reviewing the results of successive epochs in training, the loss on your validation dataset should always be decreasing. When it starts to increase, that is a sign of your model overfitting to the training dataset. . Fine-tuning the model . We will fine-tune our model to our task in the following steps: . Select the number of epochs for which we will train the model | Choose a base learning rate based on the results from the LRFinder plot above | Run the cell to initiate model training utilizing the fine_tune() method from fastai . Tip: If you&#8217;re running this notebook in Colab, you can re-run this cell with different hyperparameters to better understand how they affect the result. | epochs = 5 #@param {type: &quot;integer&quot;} base_lr = 2e-3 #@param {type: &quot;number&quot;} learn = cnn_learner(dls, resnet18, metrics=accuracy) learn.fine_tune(epochs, base_lr=base_lr) . epoch train_loss valid_loss accuracy time . 0 | 1.106555 | 1.336308 | 0.500000 | 00:01 | . epoch train_loss valid_loss accuracy time . 0 | 0.124234 | 0.632943 | 0.700000 | 00:01 | . 1 | 0.175012 | 0.107626 | 0.900000 | 00:01 | . 2 | 0.129196 | 0.022088 | 1.000000 | 00:01 | . 3 | 0.100126 | 0.012920 | 1.000000 | 00:01 | . 4 | 0.078639 | 0.012502 | 1.000000 | 00:01 | . Review training curves . The visual representation of the training and validation losses are useful to evaluate how successfully you were able to train your model. You should see the validation loss continuously decreasing over subsequent batches. . Important: If the validation loss begins to increase, your model may be starting to overfit. Consider restarting your training experiment with one fewer epochs than it took to overfit. . learn.recorder.plot_loss() . Testing the Model . Test the model on the test dataset . When you run the following cell, the first line shows the groundtruth for whether the radiograph is of the chest or abdomen. The second line is the model prediction for whether the image is a chest or abdominal radiograph. . test_files = get_image_files(path/&#39;test&#39;) test_dl = learn.dls.test_dl(test_files, with_labels=True) learn.show_results(dl=test_dl) . A little more detail on the predictions . Running this cell will provide us with the loss on each image, as well as the model&#39;s predicted probability, which can be thought of as the model&#39;s confidence in its prediction. . Note: If the model is correct and completely confident, the loss should be near &quot;0.00&quot; and the probability will be &quot;1.00&quot;, respectively. . interp = ClassificationInterpretation.from_learner(learn, dl=test_dl) interp.plot_top_losses(k=2) . Test the model on a surprise example . Here, we present the model with an unexpected image (an elbow radiograph) and see how it responds. . y = get_image_files(path, recurse=False) test_dl = learn.dls.test_dl(y) x, = first(test_dl) res = learn.get_preds(dl=test_dl, with_decoded=True) x_dec = TensorImage(dls.train.decode((x,))[0][0]) fig, ax = plt.subplots() fig.suptitle(&#39;Prediction / Probability&#39;, fontsize=14, fontweight=&#39;bold&#39;) x_dec.show(ctx=ax) ax.set_title(f&#39;{dls.vocab[res[2][0]]} / {max(res[0][0]):.2f}&#39;); . When presented with this radiograph of an elbow, the model makes a prediction but is less confident than with the other test images. . Important: (1) A deep learning classification model can only learn what we teach it to learn. . Important: (2) In designing our model implementation, we might consider designing a pre-processing step in which the data (or metadata) is checked to ensure the input to the model is valid. . Visualizing Model Inferences . Class activation map (CAM) . CAM allows one to visualize which regions of the original image are heavily weighted in the prediction of the corresponding class. This technique provides a visualization of the activations in the final convolutional block of a Convolutional Neural Network (CNN). . CAM can also be useful to determine if the model is &quot;cheating&quot; and looking somewhere it shouldn&#39;t be to make its prediction (i.e. radioopaque markers placed by the technologist). . Note: If you are running this cell in Colab, choose which of the two test images you would like to examine and run this cell to see the CAM output overlayed on the input image. . test_case = &#39;chest&#39; #@param [&#39;abd&#39;, &#39;chest&#39;] cls = 0 if test_case == &#39;abd&#39; else 1 label = test_case y = get_image_files(path/&#39;test&#39;/label) test_dl = learn.dls.test_dl(y, with_labels=True) hook = hook_output(learn.model[0]) x, _ = first(test_dl) with torch.no_grad(): output = learn.model.eval()(x) act = hook.stored[0] cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[1][-1].weight, act) x_dec = TensorImage(dls.train.decode((x,))[0][0]) _, ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map[cls].detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); hook.remove() . Grad-CAM . Gradient-weighted CAM (Grad-CAM) allows us to visualize the output from any convolutional block in a CNN. . By default, this cell is setup to show the Grad-CAM output from the final convolutional block in the CNN, for comparison to the CAM output. . Note: If you&#8217;re running this notebook in Colab, (1) choose which of the two test images you would like to examine and run this cell to see the Grad-CAM output overlayed on the input image, then (2) select a different block and re-run the cell to see how the output changes for different blocks in the network. . test_case = &#39;abd&#39; #@param [&#39;abd&#39;, &#39;chest&#39;] cls = 0 if test_case == &#39;abd&#39; else 1 label = test_case y = get_image_files(path/&#39;test&#39;/label) test_dl = learn.dls.test_dl(y, with_labels=True) x, _ = first(test_dl) mod = learn.model[0] block = -2 #@param {type: &quot;slider&quot;, min: -8, max: -1, step: 1} hook_func = lambda m,i,o: o[0].detach().clone() with Hook(mod[block], hook_func, is_forward=False) as hookg: with Hook(mod[block], hook_func) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0, cls].backward() grad = hookg.stored w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) x_dec = TensorImage(dls.train.decode((x,))[0][0]) _, ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); .",
            "url": "https://radml.wfwiggins.com/image-classification/tutorial/2021/05/01/Image_Classification_Tutorial.html",
            "relUrl": "/image-classification/tutorial/2021/05/01/Image_Classification_Tutorial.html",
            "date": " • May 1, 2021"
        }
        
    
  
    
        ,"post1": {
            "title": "Hands-on AI for Non-Coders: Image Classification Basics for Beginners",
            "content": ". . Introduction . In this demonstration, we will utilize techniques of computer vision, including deep convolutional neural networks (CNNs), to train an image classifier model capable of classifying radiographs as either chest or abdominal. . Code . We will utilize the fast.ai v2 library, written primarily by Jeremy Howard and Sylvain Gugger (with help from many others). It is written in the Python programming language and built on top of the PyTorch deep learning library. . The demonstration in this notebook relies heavily on examples from the fast.ai book, Deep Learning for Coders with fastai and PyTorch: AI Applications without a PhD by Jeremy Howard and Sylvain Gugger, which was written entirely in Jupyter notebooks, which are freely available for download on GitHub. A print copy of the book can be purchased from Amazon. . Data . This work is adapted from &quot;Hello World Deep Learning in Medical Imaging 1&quot;. The chest and abdominal radiographs were obtained from Paras Lakhani&#39;s GitHub repository. . 1. Reference: Lakhani P, Gray DL, Pett CR, Nagy P, Shih G. Hello World Deep Learning in Medical Imaging. J Digit Imaging. 2018 Jun; 31(3):283-289. Published online 2018 May 3. doi: 10.1007/s10278-018-0779-6↩ . Developers . Walter F. Wiggins, MD, PhD - Duke University Hospital, Durham, NC, USA | Kirti Magudia, MD, PhD, - University of California, San Francisco, CA, USA | M. Travis Caton, MD, PhD - University of California, San Francisco, CA, USA | . Acknowledgements . Other versions of this notebook implemented on the Kaggle Notebooks platform were presented at the 2019 Society for Imaging Informatics in Medicine (SIIM) Annual Meeting and for the American College of Radiology (ACR) Residents &amp; Fellows Section (RFS) AI Journal Club. . We would also like to acknowledge the following individuals for inspiring our transition to the Google Colab platform with their excellent notebook from the 2019 RSNA AI Refresher Course: . Luciano M. Prevedello, MD, PhD | Felipe C. Kitamura, MD, MSc | Igor Santos, MD | Ian Pan, MD | . System Setup &amp; Downloading the Data . . Important: Save a copy of this notebook in your Google Drive folder by selecting Save a Copy in Drive from the File menu in the top left corner of this page. This will allow you to modify the cells and save your results. . Warning: Make sure you have the runtime type set to &quot;GPU&quot;. See GIF below. . . Setting up the runtime environment... . Running the following cell in Colab will install the necessary libraries, download the data and restart the session. . Warning: This will generate an error message, which we can safely ignore 😉. . import os !pip install fastai==2.1.4 &gt;/dev/null !pip install fastcore==1.3.1 &gt;/dev/null # **Downloading the data...** !wget -q https://github.com/wfwiggins/RSNA-Image-AI-2020/blob/master/data.zip?raw=true !mkdir -p data !unzip -o data.zip?raw=true -d data &gt;/dev/null !rm data.zip?raw=true os.kill(os.getpid(), 9) . Exploring the Data . Let&#39;s take a look at the directory structure and contents, then create some variables to help us as we proceed. . import warnings warnings.simplefilter(&#39;ignore&#39;) import matplotlib.pyplot as plt plt.rcParams[&quot;figure.figsize&quot;] = (12, 12) . . from fastai.basics import * from fastai.vision.all import * # Set path variable to the directory where the data is located path = Path(&#39;/content/data&#39;) # Command line &quot;magic&quot; command to show directory contents !ls {path}/**/* . /content/data/test/abd: abd_test.png /content/data/test/chest: chest_test.png /content/data/train/abd: abd0.png abd14.png abd19.png abd23.png abd28.png abd3.png abd8.png abd10.png abd15.png abd1.png abd24.png abd29.png abd4.png abd9.png abd11.png abd16.png abd20.png abd25.png abd2.png abd5.png abd12.png abd17.png abd21.png abd26.png abd30.png abd6.png abd13.png abd18.png abd22.png abd27.png abd31.png abd7.png /content/data/train/chest: chst33.png chst39.png chst45.png chst51.png chst57.png chst63.png chst34.png chst40.png chst46.png chst52.png chst58.png chst64.png chst35.png chst41.png chst47.png chst53.png chst59.png chst65.png chst36.png chst42.png chst48.png chst54.png chst60.png chst37.png chst43.png chst49.png chst55.png chst61.png chst38.png chst44.png chst50.png chst56.png chst62.png /content/data/val/abd: abd0.png abd1.png abd2.png abd3.png abd4.png /content/data/val/chest: chst0.png chst1.png chst2.png chst3.png chst4.png . As you can see, the data directory contains subdirectories train, val and test, which contain the training, validation and test data for our experiment. train and val contain subdirectories abd and chest containing abdominal and chest radiographs for each data set. There are 65 training images and 10 validation images with balanced distributions over our target classes (i.e. approximately equal numbers of abdominal and chest radiographs in each data set and optimized for a classification problem). . Model Training Setup . Before we train the model, we have to get the data in a format such that it can be presented to the model for training. . Data Loaders . The first step is to load the data for the training and validation datasets into a ImageDataLoaders object from the fastai library. When training a model, the ImageDataLoaders will present training - and subsequently, validation - data to the model in batches. . Data Augmentation . In order to be sure that the model isn&#39;t simply &quot;memorizing&quot; the training data, we will augment the data by randomly applying different transformations to each image before it is sent to the model. . Transformations can include rotation, translation, flipping, rescaling, etc. . Load the data into ImageDataLoaders with data augmentation . . Note: When you run this next cell in Colab, a batch of data will be shown with or without augmentation transforms applied. (1) Run this cell once with the box next to apply_transforms unchecked to see a sample of the original images. (2) Next, run the cell a few more times after checking the box next to apply_transforms to see what happens to the images when the transforms are applied. . apply_transforms = True #@param {type: &#39;boolean&#39;} if apply_transforms: flip = True max_rotate = 10.0 max_warp = 0.2 p_affine = 0.75 else: flip = False max_rotate, max_warp, p_affine = 0, 0, 0 tfms = aug_transforms( do_flip=flip, max_rotate=max_rotate, max_warp=max_warp, p_affine=p_affine, size=224, min_scale=0.75 ) dls = ImageDataLoaders.from_folder(path, valid=&#39;val&#39;, seed=42, item_tfms=Resize(460), batch_tfms=tfms, bs=16) dls.show_batch(max_n=6) . Find the optimal learning rate . The learning rate is a hyperparameter that controls how much your model adjusts in response to percieved error after each training epoch. Choosing an optimal learning rate is an optimal step in model training. . From the fastai docs: . First introduced by Leslie N. Smith in Cyclical Learning Rates for Training Neural Networks, the LRFinder trains the model with exponentially growing learning rates and stops in case of divergence.&gt; The losses are then plotted against the learning rates with a log scale. A good value for the learning rates is then either:&gt; - 1/10th of the minimum before the divergence&gt; - where the slope is the steepest . . Note:When you run this cell for the first time in a Colab session, it will download a pretrained version of the model to your workspace before running the LRFinder. . dls = ImageDataLoaders.from_folder(path, valid=&#39;val&#39;, seed=42, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75), bs=16) learn = cnn_learner(dls, resnet18, metrics=accuracy) learn.lr_find(); . Downloading: &#34;https://download.pytorch.org/models/resnet18-5c106cde.pth&#34; to /root/.cache/torch/hub/checkpoints/resnet18-5c106cde.pth . . Transfer Learning . Deep learning requires large amounts of training data to successfully train a model. . When we don&#39;t have enough data to work with for the planned task, starting with a pre-trained network that has been optimally trained on another task can be helpful. The concept of re-training a pre-trained network for a different task is called transfer learning. . Fine-tuning . In the process of re-training the model, we start by changing the final layers of the network to define the output or predictions our model will make. In order to avoid propagating too much error through the rest of the network during the initial training, we freeze the other layers of the network for the first cycle or epoch of training. Next, we open up the rest of the network for training and train for a few more epochs. This process is called fine-tuning. . Epochs and data augmentation . During each epoch, the model will be exposed to the entire dataset. Each batch of data will have our data transformations randomly applied in order to provide data augmentation. This helps to ensure that our model never sees the exact same image twice. This is important because we wouldn&#39;t want our model to simply memorize the training dataset and not converge on a generalized solution, resulting in poor performance on the validation dataset. . The loss function . In a classification task, you&#39;re either right or wrong. This binary information doesn&#39;t give us much nuance to work with when training a model. A loss function give us a numeric estimation of &quot;how wrong&quot; our model is. This gives us a target to optimize during the training process. . When reviewing the results of successive epochs in training, the loss on your validation dataset should always be decreasing. When it starts to increase, that is a sign of your model overfitting to the training dataset. . Fine-tuning the model . We will fine-tune our model to our task in the following steps: . Select the number of epochs for which we will train the model | Choose a base learning rate based on the results from the LRFinder plot above | Run the cell to initiate model training utilizing the fine_tune() method from fastai . Tip: If you&#8217;re running this notebook in Colab, you can re-run this cell with different hyperparameters to better understand how they affect the result. | epochs = 5 #@param {type: &quot;integer&quot;} base_lr = 2e-3 #@param {type: &quot;number&quot;} learn = cnn_learner(dls, resnet18, metrics=accuracy) learn.fine_tune(epochs, base_lr=base_lr) . epoch train_loss valid_loss accuracy time . 0 | 1.106555 | 1.336308 | 0.500000 | 00:01 | . epoch train_loss valid_loss accuracy time . 0 | 0.124234 | 0.632943 | 0.700000 | 00:01 | . 1 | 0.175012 | 0.107626 | 0.900000 | 00:01 | . 2 | 0.129196 | 0.022088 | 1.000000 | 00:01 | . 3 | 0.100126 | 0.012920 | 1.000000 | 00:01 | . 4 | 0.078639 | 0.012502 | 1.000000 | 00:01 | . Review training curves . The visual representation of the training and validation losses are useful to evaluate how successfully you were able to train your model. You should see the validation loss continuously decreasing over subsequent batches. . Important: If the validation loss begins to increase, your model may be starting to overfit. Consider restarting your training experiment with one fewer epochs than it took to overfit. . learn.recorder.plot_loss() . Testing the Model . Test the model on the test dataset . When you run the following cell, the first line shows the groundtruth for whether the radiograph is of the chest or abdomen. The second line is the model prediction for whether the image is a chest or abdominal radiograph. . test_files = get_image_files(path/&#39;test&#39;) test_dl = learn.dls.test_dl(test_files, with_labels=True) learn.show_results(dl=test_dl) . A little more detail on the predictions . Running this cell will provide us with the loss on each image, as well as the model&#39;s predicted probability, which can be thought of as the model&#39;s confidence in its prediction. . Note: If the model is correct and completely confident, the loss should be near &quot;0.00&quot; and the probability will be &quot;1.00&quot;, respectively. . interp = ClassificationInterpretation.from_learner(learn, dl=test_dl) interp.plot_top_losses(k=2) . Test the model on a surprise example . Here, we present the model with an unexpected image (an elbow radiograph) and see how it responds. . y = get_image_files(path, recurse=False) test_dl = learn.dls.test_dl(y) x, = first(test_dl) res = learn.get_preds(dl=test_dl, with_decoded=True) x_dec = TensorImage(dls.train.decode((x,))[0][0]) fig, ax = plt.subplots() fig.suptitle(&#39;Prediction / Probability&#39;, fontsize=14, fontweight=&#39;bold&#39;) x_dec.show(ctx=ax) ax.set_title(f&#39;{dls.vocab[res[2][0]]} / {max(res[0][0]):.2f}&#39;); . When presented with this radiograph of an elbow, the model makes a prediction but is less confident than with the other test images. . Important: (1) A deep learning classification model can only learn what we teach it to learn. . Important: (2) In designing our model implementation, we might consider designing a pre-processing step in which the data (or metadata) is checked to ensure the input to the model is valid. . Visualizing Model Inferences . Class activation map (CAM) . CAM allows one to visualize which regions of the original image are heavily weighted in the prediction of the corresponding class. This technique provides a visualization of the activations in the final convolutional block of a Convolutional Neural Network (CNN). . CAM can also be useful to determine if the model is &quot;cheating&quot; and looking somewhere it shouldn&#39;t be to make its prediction (i.e. radioopaque markers placed by the technologist). . Note: If you are running this cell in Colab, choose which of the two test images you would like to examine and run this cell to see the CAM output overlayed on the input image. . test_case = &#39;chest&#39; #@param [&#39;abd&#39;, &#39;chest&#39;] cls = 0 if test_case == &#39;abd&#39; else 1 label = test_case y = get_image_files(path/&#39;test&#39;/label) test_dl = learn.dls.test_dl(y, with_labels=True) hook = hook_output(learn.model[0]) x, _ = first(test_dl) with torch.no_grad(): output = learn.model.eval()(x) act = hook.stored[0] cam_map = torch.einsum(&#39;ck,kij-&gt;cij&#39;, learn.model[1][-1].weight, act) x_dec = TensorImage(dls.train.decode((x,))[0][0]) _, ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map[cls].detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); hook.remove() . Grad-CAM . Gradient-weighted CAM (Grad-CAM) allows us to visualize the output from any convolutional block in a CNN. . By default, this cell is setup to show the Grad-CAM output from the final convolutional block in the CNN, for comparison to the CAM output. . Note: If you&#8217;re running this notebook in Colab, (1) choose which of the two test images you would like to examine and run this cell to see the Grad-CAM output overlayed on the input image, then (2) select a different block and re-run the cell to see how the output changes for different blocks in the network. . test_case = &#39;abd&#39; #@param [&#39;abd&#39;, &#39;chest&#39;] cls = 0 if test_case == &#39;abd&#39; else 1 label = test_case y = get_image_files(path/&#39;test&#39;/label) test_dl = learn.dls.test_dl(y, with_labels=True) x, _ = first(test_dl) mod = learn.model[0] block = -2 #@param {type: &quot;slider&quot;, min: -8, max: -1, step: 1} hook_func = lambda m,i,o: o[0].detach().clone() with Hook(mod[block], hook_func, is_forward=False) as hookg: with Hook(mod[block], hook_func) as hook: output = learn.model.eval()(x.cuda()) act = hook.stored output[0, cls].backward() grad = hookg.stored w = grad[0].mean(dim=[1,2], keepdim=True) cam_map = (w * act[0]).sum(0) x_dec = TensorImage(dls.train.decode((x,))[0][0]) _, ax = plt.subplots() x_dec.show(ctx=ax) ax.imshow(cam_map.detach().cpu(), alpha=0.6, extent=(0,224,224,0), interpolation=&#39;bilinear&#39;, cmap=&#39;magma&#39;); .",
            "url": "https://radml.wfwiggins.com/image-classification/tutorial/2020/11/15/Image_Classification_Tutorial.html",
            "relUrl": "/image-classification/tutorial/2020/11/15/Image_Classification_Tutorial.html",
            "date": " • Nov 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master- badges: true- comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://radml.wfwiggins.com/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://radml.wfwiggins.com/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://radml.wfwiggins.com/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://radml.wfwiggins.com/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}